{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pip","metadata":{}},{"cell_type":"code","source":"# ref https://www.sbert.net/examples/unsupervised_learning/SimCSE/README.html, https://www.kaggle.com/code/andtaichi/finetunig-sentencetransformer/notebook\n!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:08:10.519628Z","iopub.execute_input":"2023-01-31T14:08:10.520537Z","iopub.status.idle":"2023-01-31T14:08:24.956388Z","shell.execute_reply.started":"2023-01-31T14:08:10.520437Z","shell.execute_reply":"2023-01-31T14:08:24.954966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\n\nimport math\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\n\nfrom datasets import Dataset\nfrom sentence_transformers import SentenceTransformer, InputExample\nfrom sentence_transformers import models, losses\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_lightning import seed_everything\n\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-31T14:08:24.961397Z","iopub.execute_input":"2023-01-31T14:08:24.962086Z","iopub.status.idle":"2023-01-31T14:08:30.485912Z","shell.execute_reply.started":"2023-01-31T14:08:24.962046Z","shell.execute_reply":"2023-01-31T14:08:30.484902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"cfg = {\n    \"general\": {\n        \"input_path\": \"/kaggle/input/learning-equality-curriculum-recommendations\", \n        \"output_path\": \"./output\",\n        \"seed\": 42,\n        \"fold_list\": [0],\n        \"cv\": True,\n    },\n    \n    \"bi_encoder\": {\n        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n        \"max_length\": 128,\n        \"batch_size\": 128,\n        \"gradient_checkpointing\": False,\n        \"pooling\": \"mean\",\n        \"warmup_ratio\": 0.1,\n        \"epoch\": 5,\n        \"lr\": 5e-5,\n        \"weight_decay\": 1e-2,\n        \"freeze_layers\": 6,\n        \"reinit_layers\": 1,\n    },\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:08:30.487653Z","iopub.execute_input":"2023-01-31T14:08:30.488402Z","iopub.status.idle":"2023-01-31T14:08:30.497655Z","shell.execute_reply.started":"2023-01-31T14:08:30.488366Z","shell.execute_reply":"2023-01-31T14:08:30.496817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function","metadata":{}},{"cell_type":"code","source":"# define some helper functions and classes to aid with data traversal\n\ndef print_markdown(md):\n    display(Markdown(md))\n\nclass Topic:\n    def __init__(self, topic_id):\n        self.id = topic_id\n\n    @property\n    def parent(self):\n        parent_id = topics_df.loc[self.id].parent\n        if pd.isna(parent_id):\n            return None\n        else:\n            return Topic(parent_id)\n\n    @property\n    def ancestors(self):\n        ancestors = []\n        parent = self.parent\n        while parent is not None:\n            ancestors.append(parent)\n            parent = parent.parent\n        return ancestors\n\n    @property\n    def siblings(self):\n        if not self.parent:\n            return []\n        else:\n            return [topic for topic in self.parent.children if topic != self]\n\n    @property\n    def content(self):\n        if self.id in correlations_df.index:\n            return [ContentItem(content_id) for content_id in correlations_df.loc[self.id].content_ids.split()]\n        else:\n            return tuple([]) if self.has_content else []\n\n    def get_breadcrumbs(self, separator=\" | \", include_self=True, include_root=True):\n        ancestors = self.ancestors\n        if include_self:\n            ancestors = [self] + ancestors\n        if not include_root:\n            ancestors = ancestors[:-1]\n        return separator.join(reversed([a.title for a in ancestors]))\n\n    @property\n    def children(self):\n        return [Topic(child_id) for child_id in topics_df[topics_df.parent == self.id].index]\n\n    def subtree_markdown(self, depth=0):\n        markdown = \"  \" * depth + \"- \" + self.title + \"\\n\"\n        for child in self.children:\n            markdown += child.subtree_markdown(depth=depth + 1)\n        for content in self.content:\n            markdown += (\"  \" * (depth + 1) + \"- \" + \"[\" + content.kind.title() + \"] \" + content.title) + \"\\n\"\n        return markdown\n\n    def __eq__(self, other):\n        if not isinstance(other, Topic):\n            return False\n        return self.id == other.id\n\n    def __getattr__(self, name):\n        return topics_df.loc[self.id][name]\n\n    def __str__(self):\n        return self.title\n    \n    def __repr__(self):\n        return f\"<Topic(id={self.id}, title=\\\"{self.title}\\\")>\"\n\n\nclass ContentItem:\n    def __init__(self, content_id):\n        self.id = content_id\n\n    @property\n    def topics(self):\n        return [Topic(topic_id) for topic_id in topics_df.loc[correlations_df[correlations_df.content_ids.str.contains(self.id)].index].index]\n\n    def __getattr__(self, name):\n        return content_df.loc[self.id][name]\n\n    def __str__(self):\n        return self.title\n    \n    def __repr__(self):\n        return f\"<ContentItem(id={self.id}, title=\\\"{self.title}\\\")>\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ContentItem):\n            return False\n        return self.id == other.id\n\n    def get_all_breadcrumbs(self, separator=\" | \", include_root=True):\n        breadcrumbs = []\n        for topic in self.topics:\n            new_breadcrumb = topic.get_breadcrumbs(separator=separator, include_root=include_root)\n            if new_breadcrumb:\n                new_breadcrumb = new_breadcrumb + separator + self.title\n            else:\n                new_breadcrumb = self.title\n            breadcrumbs.append(new_breadcrumb)\n        return breadcrumbs","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-31T14:08:30.500411Z","iopub.execute_input":"2023-01-31T14:08:30.500930Z","iopub.status.idle":"2023-01-31T14:08:30.521993Z","shell.execute_reply.started":"2023-01-31T14:08:30.500893Z","shell.execute_reply":"2023-01-31T14:08:30.521016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_df = pd.read_csv(f\"{cfg['general']['input_path']}/content.csv\", index_col=0)\ncorrelations_df = pd.read_csv(f\"{cfg['general']['input_path']}/correlations.csv\", index_col=0)\ntopics_df = pd.read_csv(f\"{cfg['general']['input_path']}/topics.csv\", index_col=0)\nsub_df = pd.read_csv(f\"{cfg['general']['input_path']}/sample_submission.csv\")\nfold = pd.read_csv(f\"/kaggle/input/lecr-cv-4/fold.csv\")\n\n# fillna titles\ncontent_df[\"title\"].fillna(\"\", inplace = True)\ntopics_df[\"title\"].fillna(\"\", inplace = True)\n# fillna descriptions\ncontent_df[\"description\"].fillna(\"\", inplace = True)\ntopics_df[\"description\"].fillna(\"\", inplace = True)\n# fillna text\ncontent_df[\"text\"].fillna(\"\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:08:30.524889Z","iopub.execute_input":"2023-01-31T14:08:30.525180Z","iopub.status.idle":"2023-01-31T14:08:52.106462Z","shell.execute_reply.started":"2023-01-31T14:08:30.525140Z","shell.execute_reply":"2023-01-31T14:08:52.105304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_breadcrumbs(row):\n    topic = Topic(row.name)\n    breadcrumbs = topic.get_breadcrumbs()\n    row[\"breadcrumbs\"] = breadcrumbs\n    return row\n\ndef get_topic_inputs(row):\n    row[\"topic_inputs\"] = row[\"topic_title\"] + \" [T_SEP] \" + row[\"topic_breadcrumbs\"] + \" [T_SEP] \" + row[\"topic_description\"]\n    return row\n\ndef get_content_inputs(row):\n    row[\"content_inputs\"] = row[\"content_title\"] + \" [C_SEP] \" + row[\"content_description\"] + \" [C_SEP] \" + row[\"content_text\"].split(\"\\n\")[0]\n    return row","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:08:52.107860Z","iopub.execute_input":"2023-01-31T14:08:52.108258Z","iopub.status.idle":"2023-01-31T14:08:52.116215Z","shell.execute_reply.started":"2023-01-31T14:08:52.108221Z","shell.execute_reply":"2023-01-31T14:08:52.115219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics_df[\"breadcrumbs\"] = np.nan\ntopics_df = topics_df.progress_apply(get_breadcrumbs, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:08:52.117897Z","iopub.execute_input":"2023-01-31T14:08:52.118295Z","iopub.status.idle":"2023-01-31T14:10:25.387201Z","shell.execute_reply.started":"2023-01-31T14:08:52.118260Z","shell.execute_reply":"2023-01-31T14:10:25.386200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics_df[\"id\"] = topics_df.index\ntopics_df = topics_df.reset_index(drop=True) \n\ncontent_df[\"id\"] = content_df.index\ncontent_df = content_df.reset_index(drop=True)\n\ncorrelations_df[\"topic_id\"] = correlations_df.index\ncorrelations_df = correlations_df.reset_index(drop=True) ","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:10:25.388638Z","iopub.execute_input":"2023-01-31T14:10:25.389577Z","iopub.status.idle":"2023-01-31T14:10:25.503742Z","shell.execute_reply.started":"2023-01-31T14:10:25.389537Z","shell.execute_reply":"2023-01-31T14:10:25.502632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reinit_layer(model):\n    print(f\"reinit layer -> {cfg['bi_encoder']['reinit_layers']}\")\n    for layer in model.encoder.layer[\n        -cfg[\"bi_encoder\"][\"reinit_layers\"] :\n    ]:\n        for module in layer.modules():\n            if isinstance(module, nn.Linear):\n                module.weight.data.normal_(\n                    mean=0.0, std=model.config.initializer_range\n                )\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.Embedding):\n                module.weight.data.normal_(\n                    mean=0.0, std=model.config.initializer_range\n                )\n                if module.padding_idx is not None:\n                    module.weight.data[module.padding_idx].zero_()\n            elif isinstance(module, nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:10:25.505578Z","iopub.execute_input":"2023-01-31T14:10:25.505962Z","iopub.status.idle":"2023-01-31T14:10:25.514166Z","shell.execute_reply.started":"2023-01-31T14:10:25.505925Z","shell.execute_reply":"2023-01-31T14:10:25.513050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_sentences, fold_n):\n    model_name = cfg[\"bi_encoder\"][\"model_name\"]\n    word_embedding_model = models.Transformer(\n        cfg[\"bi_encoder\"][\"model_name\"],\n        max_seq_length=cfg[\"bi_encoder\"][\"max_length\"],\n        model_args={\"gradient_checkpointing\": cfg[\"bi_encoder\"][\"gradient_checkpointing\"]})\n    tokens = [\"[T_SEP]\", \"[C_SEP]\"]\n    word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n    word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), cfg[\"bi_encoder\"][\"pooling\"])\n    \n    # freeze\n    if cfg[\"bi_encoder\"][\"freeze_layers\"] > 0:\n        print(f\"freeze layer -> {cfg['bi_encoder']['freeze_layers']}\")\n        word_embedding_model.auto_model.embeddings.requires_grad_(False)\n        word_embedding_model.auto_model.encoder.layer[:6].requires_grad_(False)\n    \n    # reinit some layers\n    if cfg[\"bi_encoder\"][\"reinit_layers\"] > 0:\n        reinit_layer(word_embedding_model.auto_model)\n        \n    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n    # DataLoader to batch your data\n    train_dataloader = DataLoader(train_sentences, batch_size=cfg[\"bi_encoder\"][\"batch_size\"], shuffle=True, drop_last=True)\n\n    # Use the denoising auto-encoder loss\n    train_loss = losses.MultipleNegativesRankingLoss(model)\n\n    warmup_steps = math.ceil(len(train_dataloader) * cfg[\"bi_encoder\"][\"warmup_ratio\"])\n    # Call the fit method\n    model.fit(\n        train_objectives=[(train_dataloader, train_loss)],\n        epochs=cfg[\"bi_encoder\"][\"epoch\"],\n        warmup_steps=warmup_steps,\n        optimizer_params={\"lr\": cfg[\"bi_encoder\"][\"lr\"]},\n        weight_decay=cfg[\"bi_encoder\"][\"weight_decay\"],\n        show_progress_bar=True,\n        use_amp=True,\n    )\n\n    model.save(f\"{cfg['general']['output_path']}/simcse-model_fold{fold_n}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:10:25.517873Z","iopub.execute_input":"2023-01-31T14:10:25.518378Z","iopub.status.idle":"2023-01-31T14:10:25.529748Z","shell.execute_reply.started":"2023-01-31T14:10:25.518343Z","shell.execute_reply":"2023-01-31T14:10:25.528778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if cfg[\"general\"][\"cv\"]:\n    for fold_n in cfg[\"general\"][\"fold_list\"]:\n        seed_everything(cfg[\"general\"][\"seed\"], workers=True)\n        topics_df_fold = topics_df.merge(fold, how=\"inner\", left_on=\"id\", right_on=\"topic_id\")\n        topics_df_fold = topics_df_fold.drop([\"topic_id\"], axis=1)\n        topics_df_fold = topics_df_fold[topics_df_fold[\"fold\"]!=fold_n]\n\n        topics_df_fold.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n        content_df.rename(columns=lambda x: \"content_\" + x, inplace=True)\n\n        correlations = correlations_df.copy()\n        correlations.content_ids = correlations.content_ids.str.split()\n        correlations = correlations.explode(\"content_ids\").rename(columns={\"content_ids\": \"content_id\"})\n        correlations = correlations.merge(topics_df_fold, how=\"inner\", on=\"topic_id\")\n        correlations = correlations.merge(content_df, how=\"left\", on=\"content_id\")\n        correlations[\"topic_inputs\"] = np.nan\n        correlations[\"content_inputs\"] = np.nan\n        correlations = correlations.progress_apply(get_topic_inputs, axis=1)\n        correlations = correlations.progress_apply(get_content_inputs, axis=1)\n        correlations[\"set\"] = correlations[[\"topic_inputs\", \"content_inputs\"]].values.tolist()\n        train_df = pd.DataFrame(correlations[[\"set\"]])\n\n        dataset = Dataset.from_pandas(train_df)\n        train_sentences = []\n        set_v = dataset[\"set\"]\n        n_examples = dataset.num_rows\n        for i in range(n_examples):\n            _set = set_v[i]\n            train_sentences.append(InputExample(texts=[str(_set[0]), str(_set[1])]))\n\n        train(train_sentences, fold_n)\nelse:\n    seed_everything(42, workers=True)\n    fold_n = \"_all\"\n    topics_df.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content_df.rename(columns=lambda x: \"content_\" + x, inplace=True)\n\n    correlations = correlations_df.copy()\n    correlations.content_ids = correlations.content_ids.str.split()\n    correlations = correlations.explode(\"content_ids\").rename(columns={\"content_ids\": \"content_id\"})\n    correlations = correlations.merge(topics_df, how=\"inner\", on=\"topic_id\")\n    correlations = correlations.merge(content_df, how=\"left\", on=\"content_id\")\n    correlations[\"topic_inputs\"] = np.nan\n    correlations[\"content_inputs\"] = np.nan\n    correlations = correlations.progress_apply(get_topic_inputs, axis=1)\n    correlations = correlations.progress_apply(get_content_inputs, axis=1)\n    correlations[\"set\"] = correlations[[\"topic_inputs\", \"content_inputs\"]].values.tolist()\n    train_df = pd.DataFrame(correlations[[\"set\"]])\n\n    dataset = Dataset.from_pandas(train_df)\n    train_sentences = []\n    set_v = dataset[\"set\"]\n    n_examples = dataset.num_rows\n    for i in range(n_examples):\n        _set = set_v[i]\n        train_sentences.append(InputExample(texts=[str(_set[0]), str(_set[1])]))\n\n    train(train_sentences, fold_n)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:10:25.531317Z","iopub.execute_input":"2023-01-31T14:10:25.531686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"set\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}